{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice NB Using ZenRows Web-Scraping Tutorial\n",
    "---\n",
    "## Douglas Krouth\n",
    "## [TUTORIAL URL](https://www.zenrows.com/blog/stealth-web-scraping-in-python-avoid-blocking-like-a-ninja#ip-rate-limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.62.199.225\n"
     ]
    }
   ],
   "source": [
    "# IP Rate Limit\n",
    "# The idea here is that we don't know whether there exists IP rate limits that we need to be aware of.\n",
    "# The workaround for this is to use a rotating/changing IP address via proxy.\n",
    "import requests \n",
    " \n",
    "response = requests.get('http://httpbin.org/ip') \n",
    "print(response.json()['origin'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free proxy list site : [URL](https://free-proxy-list.net/)\n",
    "---\n",
    "This gives us a list of *unreliable* proxies that we can practice with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105.242.158.92\n"
     ]
    }
   ],
   "source": [
    "proxies = {'http': 'http://105.242.158.92:3129'} \n",
    "response = requests.get('http://httpbin.org/ip', proxies=proxies) \n",
    "print(response.json()['origin']) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Agent Header\n",
    "Check request headers to see if we're presenting any suspicious UA info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python-requests/2.28.2\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('http://httpbin.org/headers') \n",
    "print(response.json()['headers']['User-Agent']) \n",
    "# python-requests/2.25.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\n"
     ]
    }
   ],
   "source": [
    "# Fake User-Agent header passed with requests\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"} \n",
    "response = requests.get('http://httpbin.org/headers', headers=headers) \n",
    "print(response.json()['headers']['User-Agent']) # Mozilla/5.0 ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Agent\n",
    "Mitigate detection by rotating User-Agent headers at random. Same idea as IP Addresses, allows us to simulate varying user traffic.\n",
    "\n",
    "There are resources online that can be used to generate/source info on browser user-agent info. One challenge with simulating UA data is that there can be changes/updates to browsers that will need to be acommodated as they'll likely change the format of the UA header.\n",
    "\n",
    "[USER AGENT DATABASE](https://explore.whatismybrowser.com/useragents/explore/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    " \n",
    "user_agents = [ \n",
    "\t'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36', \n",
    "\t'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36', \n",
    "\t'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36', \n",
    "\t'Mozilla/5.0 (iPhone; CPU iPhone OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148', \n",
    "\t'Mozilla/5.0 (Linux; Android 11; SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.72 Mobile Safari/537.36' \n",
    "] \n",
    "user_agent = random.choice(user_agents) \n",
    "headers = {'User-Agent': user_agent} \n",
    "response = requests.get('https://httpbin.org/headers', headers=headers) \n",
    "print(response.json()['headers']['User-Agent']) \n",
    "# Mozilla/5.0 (iPhone; CPU iPhone OS 12_2 like Mac OS X) ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Browser Headers\n",
    "The use of simple User Agent header swaps is easily detected with good security systems. We need a better way to bypass this-an easy answer is to generate user-agent headers with a browser dev tool like Chrome's web dev suite. This data is usually formatted as cURL so we need to format it as JSON.\n",
    "\n",
    "These headers have obvious differences (Chrome/Chromium vs FireFox)\n",
    "\n",
    "### Chrome sample header\n",
    "\n",
    "```\n",
    "{ \n",
    "\t\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "\t\"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "\t\"Accept-Language\": \"en-US,en;q=0.9\", \n",
    "\t\"Host\": \"httpbin.org\", \n",
    "\t\"Sec-Ch-Ua\": \"\\\"Chromium\\\";v=\\\"92\\\", \\\" Not A;Brand\\\";v=\\\"99\\\", \\\"Google Chrome\\\";v=\\\"92\\\"\", \n",
    "\t\"Sec-Ch-Ua-Mobile\": \"?0\", \n",
    "\t\"Sec-Fetch-Dest\": \"document\", \n",
    "\t\"Sec-Fetch-Mode\": \"navigate\", \n",
    "\t\"Sec-Fetch-Site\": \"none\", \n",
    "\t\"Sec-Fetch-User\": \"?1\", \n",
    "\t\"Upgrade-Insecure-Requests\": \"1\", \n",
    "\t\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\" \n",
    "}\n",
    "```\n",
    "### FireFox sample header\n",
    "```\n",
    "{ \n",
    "\t\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\", \n",
    "\t\"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "\t\"Accept-Language\": \"en-US,en;q=0.5\", \n",
    "\t\"Host\": \"httpbin.org\", \n",
    "\t\"Sec-Fetch-Dest\": \"document\", \n",
    "\t\"Sec-Fetch-Mode\": \"navigate\", \n",
    "\t\"Sec-Fetch-Site\": \"none\", \n",
    "\t\"Sec-Fetch-User\": \"?1\", \n",
    "\t\"Upgrade-Insecure-Requests\": \"1\", \n",
    "\t\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:90.0) Gecko/20100101 Firefox/90.0\" \n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To get browser headers\n",
    "Visit desired website in browser with Dev Tools -> Network open, grab request header and copy it as cURL. Then convert the cURL to JSON.\n",
    "\n",
    "[cURL to JSON Converter](https://www.scrapingbee.com/curl-converter/json/)\n",
    "### Sample header from httpbin.org\n",
    "```\n",
    "{\n",
    "    \"url\": \"http://httpbin.org\",\n",
    "    \"raw_url\": \"http://httpbin.org/\",\n",
    "    \"method\": \"get\",\n",
    "    \"headers\": {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Cache-Control\": \"max-age=0\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\"\n",
    "    },\n",
    "    \"insecure\": false\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cookies\n",
    "Cookies are small blocks of data generated from web servers/hosts that are placed on a user's machine via web browser. The main benefit of cookies is that they enable stateful behaviors that can persist across browsing sessions.\n",
    "\n",
    "**Examples** : Maintain login info, user selections, payment info\n",
    "\n",
    "## Cookies w/ scraping\n",
    "Because cookies can track ind. user sessions, we need to factor in implementation/design that can use cookies provided by a site to avoid getting blocked/suspicious behavior resulting from improper cookies.\n",
    "\n",
    "Rotating proxies can cause cookie problems as sites can recognize repeated traffic from various locations that don't provide the cookie originally provided by the site for that \"browsing session\".\n",
    "\n",
    "## Whether or not to use Cookies when scraping\n",
    "Simple cases / websites? Don't worry about cookies/maintaining a session.\n",
    "\n",
    "Completx sites / lot of auth? Session cookies provide potential access/ease of implementation to mitigate security if session details (IP, User Agent) can be maintained."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playwright\n",
    "For sites that require more rigorous auth (cookies, etc.) we can use a headless browser. Tutorial uses Playwright, Selenium will also work fine.\n",
    "\n",
    "NOTE : To get Playwright to work in Jupyter, we need to use the async package instead of sync. Also, we need to install drivers/executable for desired mock browser to be used.\n",
    "\n",
    "1. ``` pip install pytest-playwright```\n",
    "\n",
    "2. ```npx install chromium```\n",
    "\n",
    "[Playwright for Python intro](https://playwright.dev/python/docs/intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playwright test\n",
    "import re\n",
    "from playwright.sync_api import Page, expect\n",
    "\n",
    "\n",
    "def test_homepage_has_Playwright_in_title_and_get_started_link_linking_to_the_intro_page(page: Page):\n",
    "    page.goto(\"https://playwright.dev/\")\n",
    "\n",
    "    # Expect a title \"to contain\" a substring.\n",
    "    expect(page).to_have_title(re.compile(\"Playwright\"))\n",
    "\n",
    "    # create a locator\n",
    "    get_started = page.get_by_role(\"link\", name=\"Get started\")\n",
    "\n",
    "    # Expect an attribute \"to be strictly equal\" to the value.\n",
    "    expect(get_started).to_have_attribute(\"href\", \"/docs/intro\")\n",
    "\n",
    "    # Click the get started link.\n",
    "    get_started.click()\n",
    "\n",
    "    # Expects the URL to contain intro.\n",
    "    expect(page).to_have_url(re.compile(\".*intro\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proxy_zenrows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25c552f3230a1693763d9531032d4a5d57dd0e4b79598c49528b66801debd5e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
